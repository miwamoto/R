---
title: "Lab 4: Does Prenatal Care Improve Infant Health?"
author: "w203 Section 4: Jessica Economou, Kevin Gifford, Mona Iwamoto"

date: "April 27, 2017"
output: pdf_document
---

# Introduction

This is a group lab.  You may work in teams of 2 or 3.

The file bwght\_w203.RData contains data from the National Center for Health Statistics and from birth certificates.  Your team has been hired by a health advocacy group to study this data and help them understand whether prenatal care improves health outcomes for newborn infants.

The file includes a birthweight variable.  Additionally, the one- and five-minute APGAR scores are included.  These are measures of the well being of infants just after birth.

Variable descriptions are provided as follows.

```{r}
library(car)
library(ggplot2)
library(reshape2)
library(stargazer)
library(lmtest)
library(sandwich)
load("bwght_w203.RData")
desc
```

# Assignment

Prepare a report addressing the question of whether prenatal care improves newborn health outcomes.

A successful submission will include

1. A brief introduction

2. A model building process, supported by exploratory analysis.  Your EDA should be interspersed with, and support, your modeling decisions.  In particular, you should use exploratory techniques to address

- What transformations to apply to variables and what new variables should be created.

- What variables should be included in each model

- Whether model assumptions are met

***

## Baby Health Outcomes

*Birth Weight:* Observations of birth weight in the sample are almost normally distributed, with a slight negative skew. Mean and median birth weight are both approximately 3.4 kg. While the lowest birth weight in the sample is very low at 360 grams, review of the data indicates that there are six observations with recorded birth weights below 1 kg. Babies born prematurely can exhibit very low birth weights, so we chose not to exclude these records from analysis.  
```{r}
summary(data$bwght)
```
```{r}
hist(data$bwght, breaks = 20, main = "Histogram of Birth Weight", ylim = c(0,300), 
     xlab = "Birth weight in grams")
```
  
*APGAR Scores:* APGAR Scores in this dataset were strongly negatively skewed. The one minute APGAR had slightly more variation than the 5 minute, as `r sum(data$fmaps == 9, na.rm = T)/length(data$fmaps) * 100` percent of the 5 minute APGAR scores were a 9, and only `r sum(data$omaps == 9, na.rm = T)/length(data$omaps) * 100` percent of 1 minute APGAR scores were a score of 9 (also the mode, but scores in this 1 minute APGAR had more of a variation). After researching the meaning of the APGAR scores, we felt it necessary to include these in our measure for baby health, as they are meant to be a quick assessment on how the baby is doing outside the womb.
Summary of 5-Minute APGAR Scores:
```{R}
summary(data$fmaps)
```
  
Summary of 1-Minute APGAR Scores:  
```{r}
summary(data$omaps)
```

```{R}
layout(matrix(c(1,2), 1, byrow = TRUE))
hist(data$omaps, main = "1 Min APGAR Scores Histogram")
hist(data$fmaps, main = "5 Min APGAR Scores Histogram")
```

*New Variable - Baby Score:* Given the nature of these variables, we defined a new variable that takes both birthweight and APGAR scores into account called `bbscore`, or Baby Score. This was calculated by adding the five minute APGAR score with a weighted one minute APGAR score and then multiplying the result with the birth weight.  We weighted the one-minute score by a factor of 80%, because doing this allowed for the 5-minute measure to hold more weight in the overall score (as the 5-minute APGAR is a better indicator on the baby's actual health, and the one-minute is more of an assessment on how the baby handled the birthing process). We did not think excluding the 1-minute score was a good idea, as this measure did have more variability and still tells of the baby's health at a certain point in time.

```{R}
data$bbscore <- ((data$fmaps + (data$omaps*0.8)) * data$bwght)
```
In addition to accounting for factors other than birth weight, one advantage of using this new measure of outcome is that this APGAR factor adds variance and spreads out the curve.  Note that the curve resembles a normal distribution.

```{R}
hist(data$bbscore, breaks = 50, main = "Histogram of Baby Health (bbscore)", xlab = "bbscore")
```
```{r}
summary(data$bbscore)
```




## Primary Predictor Variables
As described above, our modeling process focuses on using APGAR scores and birth weight to measure health outcomes. Reviewing the data available, the two variables most associated with prenatal medical care are the number of prenatal visits (npvis) and the month of the pregnancy in which prenatal care began (monpre). For purposes of modeling, we can create a new variable to measure the number of months of prenatal care received.  
```{r}
data$premonths = 9 - data$monpre
```
  
*Number of Months of Prenatal Care:* This variable is calculated from "data\$monpre", which is the month in which prenatal care began by subtracting "data\$monpre" from 9 months. The median length of prenatal care is `r round(median(data$premonths),2)`, and the data displays a heavy negative skew, indicating that most mothers in the sample begin receiving prenatal care early in their pregnancies.  

*Number of Prenatal Visits:* This variable exhibits a wide distribution curve with a sharp spike at 12 visits. Doctors generally recommend a prenatal checkup schedule of one visit per month during the first 6-7 months of pregnancy, followed by more frequent visits in the weeks leading up to delivery. The observed spike is likely the result of a large number of mothers in the sample following this recommendation or a similar schedule. The sample also includes 26 observations with more than 20 prenatal visits, up to a maximum of 40 visits. These outliers could represent difficult pregnancies that required additional medical care, errors in data collection, or both. However, these cases are not excluded from analysis because the large spike in observations at 12 visits limits the effects of these outliers on the sample average. 

```{r}
layout(matrix(c(1,2), 1, byrow = TRUE))
hist(data$npvis, breaks = seq(-0.5, 40.5,by=1))
hist(data$premonths, breaks = seq(-0.5,9.5,by=1), xlim = c(0,10))
```
  
```{r}
scatterplot(jitter(data$premonths), jitter(data$npvis))
```
```{r}
cor(data$premonths, data$npvis, use = "complete.obs")
```
Based on the plot and calculations above, there appears to be a moderate positive correlation between the number of months of prenatal care received and the number of prenatal visits. This is quite natural; expectant mothers who begin care early have more time to visit the doctor before they give birth. The two variables have a moderate correlation, so including them both as predictor variables in the same model would not strictly violate the assumption regarding perfect multicollinearity, but doing so would increase the bias of the model because some number prenatal visits could potentially be explained by duration of care.  

We did notice that there were two cases where the mother started getting prenatal care very late (in the last month of her pregnancy), yet visited the doctor 30 times. We thought this was strange, so we analyzed these two data points closer to see if we found anything else strange about these two data points that would require them to be removed:

```{r}
data[!is.na(data$npvis) & data$npvis >=30 & data$premonths == 1,]
```
These don't appear to be "bad data" points judging by the rest of the data, so we made the decision to leave them in, yet be wary of their impact as possible outliers.

Doctors also commonly recommend that mothers avoid smoking and drinking during their pregnancy as a standard prenatal health practice. For this reason, we decided to analyze these two variables individually as well.
```{r}
layout(matrix(c(1,2), 1, byrow = TRUE))
hist(data$cigs, breaks = 10, labels = TRUE, ylim = c(0,2000), main = "Histogram of Cigarettes",
     xlab = "Cigarettes Smoked per Day")
hist(log(data$cigs), breaks = 10, ylim = c(0,60), labels = TRUE, main = "Histogram of Log of Cigarettes",
     xlab = "log of Cigarettes Smoked per Day")
```
  
As shown in the histograms above, the _cigs_ variable displays a heavy positive skew, as relatively few mothers in the sample smoked during their pregnancies, resulting in an extremely non-normal distribution. A more normal distribution can be achieved by applying a logarithmic transformation to the data.

```{r}
scatterplot(jitter(data$cigs), jitter(data$bwght))
```

```{r}
cor(data$cigs, data$bwght, use = "complete.obs")
```
As shown above, the number of cigarettes smoked per day displays a weak negative correlation with birth weight.  

```{r}
hist(data$drink, breaks = seq(-0.5,8.5,by=1), ylim = c(0,2000), labels = TRUE, 
     main = "Histogram of Drinks per Week", xlab = "Alcoholic Drinks per Week")
```
```{r}
scatterplot(jitter(data$drink), jitter(data$bwght))
```
```{r}
cor(data$drink, data$bwght, use = "complete.obs")
```
  
  
## Additional Predictor Variables
To identify additional potential predictor variables, we can examine a correlation heat map.  

```{r}
qplot(x=Var1, y=Var2, data=melt(cor(data, use="p")), fill=value, geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1))
```
  
Examining the heat map for correlations, we see that there appear to be positive correlations between birth weight and the age of the mother (_mage_), white fathers (_fwhte_), and the father's education (_feduc_).  

```{r}
layout(matrix(c(1,2,3,0), 2, byrow = TRUE))
hist(data$mage, breaks = seq(-0.5, 44.5,by=1), xlim = c(0,50), main = "Histogram of Mother's Age",
     xlab = "Mother's age in years")
hist(data$fwhte, breaks = seq(-0.5, 1.5,by=1), main = "Histogram of White Fathers",
     xlab = "1 = White, 0 = Non-White")
hist(data$feduc, breaks = seq(-0.5, 18.5, by=1), xlim = c(0,20), main = "Histogram of Father's Education",
     xlab = "Father's education in years")
```
  
```{r}
scatterplot(jitter(data$mage), jitter(data$bwght))
scatterplot(jitter(data$fwhte), jitter(data$bwght))
scatterplot(jitter(data$feduc), jitter(data$bwght))
```
 
```{r}
cor(data$mage, data$bwght, use = "complete.obs")
cor(data$fwhte, data$bwght, use = "complete.obs")
cor(data$feduc, data$bwght, use = "complete.obs")
```

***

3. A minimum of three model specifications.  In particular, you should include

- One model with only the explanatory variables of key interest.
## Modeling Healthy Outcomes
### Model 1

For our initial model, our explanatory variables are `npvis`  - number for primary care visits, `cigs`  - number of cigarettes smoked per day and `drink` - average number of drinks per week. We also excluded any mothers under the age of 18 and over the age of 35, as there are widely known health complications within these ages. It's important to note that because of this restriction, our model provided is for women over age 18 and under age 35 only.

```{R}
data1 <- data[data$mage > 18 & data$mage < 35,]
data1 = na.exclude(data1, complete.cases(data1))

model1 <- lm(bbscore ~ npvis + cigs + drink, data = data1)
model1
```

```{r}
coeftest(model1, vcov = vcovHC)
```


```{r}
layout(matrix(c(1,2,3,4), 2, byrow = TRUE))
plot(model1)
```
### Analysis of CLM Assumptions

#### CLM Assumption 1 
Linear population model - For all three models, we make the weak assumption that there is linearity in the parameters and use the linear regression model.

#### CLM Assumption 2 
Random Sampling - The supplied dataset `bwght\_w203.RData` contains data from the National Center for Health Statistics. The National Center for Health Statistics is the nationâ€™s principal health statistics agency and its data is used for health policy decisions and national research. They cite their sources and birth certificates, patient medical records, personal interviews, lab tests and facility information. The center has adopted high standards for survey design and data collection.  The methodology is described at `https://www.cdc.gov/nchs/data/factsheets/factsheet_health_statistics.htm` 
Based on this information, we assume that the subset of data provided for this analysis is a valid random sample.

#### CLM Assumption 3 
No perfect Collinearity - To verify that this assumption  is valid we reexamine our variables - npvis, cigs and drink.  We create a covariance 
```{R}

M <- data.matrix(subset(data1, select=c("npvis", "cigs", "drink" ))) 
C1 <- cov(M)
C1
```
In this matrix shown above, the values along the diagonal are the variances and the other values are the covariances between the variables.  None of the variables are constant (as the variance is greater than zero.  
```{R}
cor(M)
```
Looking at the correlation matrix above we can verify that none of our variables have an exact linear relationship. 

```{r}
vif(model1)
``` 
Our low variable inflation factors (VIFs) also indicate there is no perfect multicollinearity in our model.

#### CLM Assumption 4 
Zero Conditional  Mean - For model 1, we examine the Residuals verses Fitted Values plot above.  We note that on the lower and upper extremes there are few data points which may account for slight variation from zero.  Otherwise the data indicate that this assumption holds, as the mean of the residual values are centered at 0. 

#### CLM Assumption 5 
Homoskedasticity - To verify this assumption, we again look at our residuals verses fitted plot. As evident in this plot, homoskedasticity cannot be assumed.  There appears to be more variation in the middle of the graph. So, we will run the Breusch-Pagan test.
```{R}
bptest(model1)
```
Interestingly, with a p-value of 0.1076, we cannot reject the null hypothesis of homoskedasticity.  However, with borderline statistical significance and evaluation of the residuals verses fitted plot. We choose to use robust standard error.

#### CLM Assumption 6
Normality - Here, we want to show that the population error is independent of our explanatory variables and normally distributed.
```{R}
hist(model1$residuals, main = "Histogram of Residuals", breaks = 20)
```
We see that the distribution resembles a normal distribution.  

- One model that includes only covariates that you believe increase the accuracy of your results without introducing bias.  
### Model 2  
```{R}
data = na.exclude(data1, complete.cases(data))
model2 <- lm(bbscore ~ npvis + cigs + drink + mage + male + feduc + fblck, data = data)
model2
```
  
```{r}
coeftest(model2, vcov = vcovHC)
```
  
```{r}
layout(matrix(c(1,2,3,4), 2, byrow = TRUE))
plot(model2)
```
### Analysis of CLM Assumptions

#### CLM Assumption 1 and 2
These assumptions hold as in model 1.

#### CLM Assumption 3 
No perfect Collinearity - To verify that this assumption  is valid we look at our new set of variables -  cigs, drink, mage, male, educ and fblck.  We create a covariance matrix. 
```{R}
N <- data.matrix(subset(data, select=c("npvis", "cigs", "drink", "mage", "male", "feduc", "fblck" ))) 
C2 <- cov(N)
C2
```
In this matrix shown above, the values along the diagonal are the variances and the other values are the covariances between the variables.  None of the variables are constant (as the variance is greater than zero.  
```{R}
cor(N)
```
Looking at the correlation matrix above we can verify that none of our variables have an exact linear relationship. 

```{r}
vif(model2)
```  
Our low variable inflation factors (VIFs) also indicate there is no perfect multicollinearity in our model.

#### CLM Assumption 4 
Zero Conditional Mean - For model 2, we examine the Residuals verses Fitted Values plot above.  We note that the values are centered around zero as indicated by the fitted line.  Thus, this assumption holds.

#### CLM Assumption 5 
Homoskedasticity - To verify this assumption, we again look at our residuals verses fitted plot.  In this plot,  there appears to be a nearly even thickness across the x axis. So, we will run the Breusch-Pagan test to verify homoskedasticity.
```{R}
bptest(model2)
```
With a p-value of 0.201, we cannot reject the null hypothesis of homoskedasticity.  So, this assumption holds.
#### CLM Assumption 6
Normality - As with our original model, we want to show that the population error is independent of our explanatory variables and normally distributed.
```{R}
hist(model2$residuals, main = "Histogram of Residuals", breaks = 20)
```
We see that the distribution resembles a normal distribution.  

- One model that includes the previous covariates, but also covariates that may be problematic for one reason or another. 
### Model 3  

```{R}
model3 <- lm(bbscore ~ npvis + cigs + drink + mage + male + feduc + fblck + meduc + mblck + monpre, data = data)
model3
```

```{r}
coeftest(model3, vcov = vcovHC)
```
  
```{r}
layout(matrix(c(1,2,3,4), 2, byrow = TRUE))
plot(model3)
```
### Analysis of CLM Assumptions

#### CLM Assumption 1 and 2
These assumptions hold as in model 1.

#### CLM Assumption 3 
No perfect Collinearity - To verify that this assumption  is valid we look at our new set of variables -  npvis, cigs, drink, mage, male, feduc, fblck,  meduc, mblck and monpre.  We create a covariance matrix. 
```{R}
O<- data.matrix(subset(data, select=c("npvis", "cigs", "drink", "mage", "male", "feduc", "fblck", "meduc", "mblck", "monpre" ))) 
C3 <- cov(O)
C3
```
In this matrix shown above, the values along the diagonal are the variances and the other values are the covariances between the variables.  None of the variables are constant (as the variance is greater than zero.)  
```{R}
cor(O)
```
Looking at the correlation matrix above we can verify that none of our variables have an exact linear relationship. 

```{r}
vif(model3)
```  
Examining our variable inflation factors we see evidence  of multicollinearity in our model. In particular, the variables fblck and mblck have high VIFs.  This may be due to a relationship between these to variables and suggests that the incidence of parents being of the same race is not random.

#### CLM Assumption 4
Zero Conditional Mean - Similar to models 1 and 2.  This assumption holds.


#### CLM Assumption 5 
Homoskedasticity - To verify this assumption, we again look at our residuals verses fitted plot.  In this plot, we see evidence of heteroskedasticity. So, we will run the Breusch-Pagan test.
```{R}
bptest(model3)
```
With a p-value of 0.03569, we reject the null hypothesis of homoskedasticity.  So, this assumption does not hold.  However with a large sample we rely on OLS asymptotics.

#### CLM Assumption 6
Normality - As with our original model, we want to show that the population error is independent of our explanatory variables and normally distributed.
```{R}
hist(model2$residuals, main = "Histogram of Residuals", breaks = 20)
```

4. For your first model, a detailed assessment of the 6 CLM assumptions.  For additional models, you should check all assumptions, but only highlight major differences from your first model in your report.


5. A well-formatted regression table summarizing your model results.  Make sure that standard errors presented in this table are valid.  Also be sure to comment on both statistical and practical significance.

```{R}
stargazer(model1, model2, model3, type = "text", omit.stat = "f",
          star.cutoffs = c(0.05, 0.01, 0.001))
```

6. A discussion of whether your results can be interpretted causally.  In particular, include a discussion of what variables are not included in your analysis and the likely direction of omitted variable bias.  Also include a discussion of which included variables may bias your results by absorbing some of the causal effect of prenatal care.

7. A brief conclusion with a few high-level takeaways.


Please limit all submissions to 30 pages.  Be sure to turn in both your pdf report and also your source code.
